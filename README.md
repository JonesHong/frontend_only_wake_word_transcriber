# üéôÔ∏è Multilingual Smart Voice Assistant

A feature-complete frontend-only voice assistant application that integrates wake word detection, voice activity detection (VAD), speech transcription, and intelligent interface design.

üåê **[Live Demo](https://joneshong.github.io/frontend_only_wake_word_transcriber/)**

## ‚ú® Features

### üîä Speech Processing
- **Multiple Wake Word Models**: Supports Hey Jarvis, Hey Mycroft, and Alexa wake words
- **Intelligent Voice Activity Detection (VAD)**: Automatically detects voice activity with support for automatic/manual ending modes
- **Real-time Speech Transcription**: Uses Web Speech API to convert speech to text
- **Audio Playback**: Records and allows replaying of audio segments

### üé® User Interface
- **Bilingual Interface**: Complete Traditional Chinese/English multilingual support
- **Dark/Light Theme**: Switchable visual themes
- **Real-time Visualization**: Waveform charts, wake word score graphs, and VAD status display
- **Responsive Design**: Supports desktop and mobile devices

### ‚öôÔ∏è Advanced Features
- **Flexible Settings System**: Adjustable silence timeout, model switching, and detection mode configuration
- **Logging System**: Complete system activity logging and debugging functionality
- **Modular Architecture**: Finite State Machine (FSM) based system state management
- **Pure Frontend Implementation**: No backend services required, deployable directly to GitHub Pages

## System Architecture

### Finite State Machine (FSM)

The application uses three states to control the flow:

1. **Initialization**: System initial state
2. **Idle**: Waiting for wake word trigger
3. **Listening**: After wake word detection, starts listening and transcribing speech

### State Transitions

- Initialization ‚Üí Idle: Press "Start Recording"
- Idle ‚Üí Listening: Wake word detected
- Listening ‚Üí Idle: No voice activity for 3 consecutive seconds
- Any State ‚Üí Initialization: Press "Stop Recording"

## üöÄ Usage

### Basic Operations
1. Open `index.html` in a supported browser (Chrome or Edge recommended)
2. Allow microphone permissions
3. Click the "‚ñ∂Ô∏è Start" button to activate the system
4. Say a wake word (Hey Jarvis, Hey Mycroft, Alexa)
5. After wake word detection, the system will play a notification sound and start listening
6. Start speaking, the system will transcribe your speech in real-time and display results
7. The system automatically returns to waiting state after speech ends

### Advanced Settings
- **Settings Button**: Click the gear icon in the bottom right to open the settings panel
  - Switch between manual/automatic ending modes
  - Adjust silence timeout (0.5-5.0 seconds)
  - Select different wake word models
- **Theme Toggle**: Click the üåì button to switch between dark/light themes
- **Language Switch**: Use the dropdown menu to switch between Traditional Chinese/English interface
- **Log Viewing**: Click the üìã button in the bottom right to view system logs

## Technical Specifications

- **Audio Sample Rate**: 16kHz
- **Audio Processing**: 80ms audio chunks (1280 samples)
- **VAD Delay**: 12-frame buffer to prevent premature cutoff
- **Wake Word Threshold**: 0.5

## üìÅ Project Structure

```
voice-assistant/
‚îú‚îÄ‚îÄ index.html          # Main HTML page
‚îú‚îÄ‚îÄ styles.css          # CSS stylesheet
‚îú‚îÄ‚îÄ js/                 # JavaScript modules
‚îÇ   ‚îú‚îÄ‚îÄ main.js         # Main program and application initialization
‚îÇ   ‚îú‚îÄ‚îÄ fsm.js          # Finite state machine logic
‚îÇ   ‚îú‚îÄ‚îÄ wakeword.js     # Wake word detection module
‚îÇ   ‚îú‚îÄ‚îÄ vad.js          # Voice activity detection module
‚îÇ   ‚îú‚îÄ‚îÄ speech.js       # Speech transcription module
‚îÇ   ‚îú‚îÄ‚îÄ visualization.js # Audio visualization charts
‚îÇ   ‚îú‚îÄ‚îÄ settings.js     # Settings system and UI
‚îÇ   ‚îú‚îÄ‚îÄ logger.js       # Logging system
‚îÇ   ‚îú‚îÄ‚îÄ theme.js        # Theme switching management
‚îÇ   ‚îú‚îÄ‚îÄ i18n.js         # Multilingual internationalization
‚îÇ   ‚îî‚îÄ‚îÄ language.js     # Language switching logic
‚îú‚îÄ‚îÄ models/             # ONNX model files
‚îÇ   ‚îú‚îÄ‚îÄ hey_jarvis_v0.1.onnx     # Hey Jarvis wake word model
‚îÇ   ‚îú‚îÄ‚îÄ hey_mycroft_v0.1.onnx    # Hey Mycroft wake word model
‚îÇ   ‚îú‚îÄ‚îÄ alexa_v0.1.onnx          # Alexa wake word model
‚îÇ   ‚îú‚îÄ‚îÄ embedding_model.onnx     # Audio feature extraction model
‚îÇ   ‚îú‚îÄ‚îÄ melspectrogram.onnx      # Spectrogram conversion model
‚îÇ   ‚îú‚îÄ‚îÄ silero_vad.onnx          # VAD detection model
‚îÇ   ‚îî‚îÄ‚îÄ whisper-tiny/             # Whisper speech transcription model (included for demo)
‚îú‚îÄ‚îÄ tools/              # Utility scripts
‚îÇ   ‚îî‚îÄ‚îÄ sync_registry_models.py  # Script to download additional models
‚îî‚îÄ‚îÄ reference/          # Reference materials and example code
```

## üì¶ Model Management

### Included Models
The repository includes essential models for GitHub Pages deployment:
- **Wake Word Models**: Hey Jarvis, Hey Mycroft, Alexa (small size, ~800KB-1.3MB each)
- **VAD Model**: Silero VAD for voice activity detection (~1.8MB)
- **Whisper Tiny Quantized**: Smallest Whisper model for speech transcription (~40MB total)
  - Provides good transcription quality for demos
  - Suitable for GitHub Pages deployment

### Downloading Additional Models
For larger Whisper models (base, small, medium, large) with better accuracy:

1. **Using the sync script** (for developers who clone the repository):
   ```bash
   # Download all models from registry
   python3 tools/sync_registry_models.py
   
   # Download specific model category
   python3 tools/sync_registry_models.py --category whisper
   ```

2. **Manual download**:
   - Visit [Hugging Face](https://huggingface.co/Xenova) for Whisper models
   - Download desired model files to `models/huggingface/Xenova/[model-name]/onnx/`
   - Update model configuration in settings as needed

### Model Size Reference
- **whisper-tiny-quantized**: ~40MB (included)
- **whisper-base-quantized**: ~75MB 
- **whisper-small-quantized**: ~195MB
- **whisper-medium**: ~1.5GB
- **whisper-large**: ~3GB

> **Note**: Only whisper-tiny-quantized is included for GitHub Pages compatibility. Larger models provide better accuracy but require local deployment due to file size constraints.

## üåê Deployment

### GitHub Pages Deployment
1. Fork or Clone this project to your GitHub account
2. In GitHub repository settings ‚Üí Pages ‚Üí Source, select "Deploy from a branch"
3. Choose the main branch
4. Wait for deployment to complete, then access via GitHub Pages URL

### Local Development
Due to browser security restrictions, it's recommended to use an HTTP server:
```bash
# Using Python 3
python -m http.server 8000

# Using Node.js http-server
npx http-server

# Or use Live Server extension (VS Code)
```

## üñ•Ô∏è Browser Compatibility

| Browser | Support Level | Notes |
|---------|---------------|-------|
| Chrome | ‚úÖ Full Support | Recommended, all features complete |
| Edge | ‚úÖ Full Support | Chromium-based, full functionality |
| Firefox | ‚ö†Ô∏è Partial Support | Limited Web Speech API functionality |
| Safari | ‚ö†Ô∏è Partial Support | Some WebRTC features may be restricted |

## ‚ö†Ô∏è Important Notes

### Requirements
- **Microphone Permission**: Application requires microphone access
- **HTTPS Protocol**: Some browser APIs require secure connections
- **WebAssembly Support**: ONNX Runtime requires WASM support
- **Network Connection**: Web Speech API requires internet connectivity

### Usage Recommendations
- Use in quiet environments for optimal wake word detection
- Maintain appropriate volume and clarity when speaking
- Check the logging system for detailed information if issues occur

## üõ†Ô∏è Technical Highlights

- **Pure Frontend Implementation**: No backend services required, runs entirely in the browser
- **Modular Design**: Each functional module is independent, easy to maintain and extend
- **State Management**: Uses finite state machine to ensure consistent system behavior
- **Performance Optimization**: Uses WebAssembly to accelerate AI model inference
- **Accessibility Design**: Supports keyboard navigation and screen readers

## üôè Acknowledgments and References

This project's implementation benefits from the following excellent open-source projects and technical articles:

### Core Technical References
- **[Open Wake Word on the Web](https://deepcorelabs.com/open-wake-word-on-the-web/)** by Miro Hristov  
  In-depth technical article detailing how to port a Python wake word system to the browser, including key audio processing pipeline architecture and debugging experience

- **[VAD (Voice Activity Detection)](https://github.com/ricky0123/vad)**  
  Provides browser-based voice activity detection implementation examples and best practices

- **[Web Speech API](https://www.google.com/intl/en/chrome/demos/speech.html)**  
  Google Chrome's official speech recognition API demonstration, providing the foundation for speech-to-text functionality

### Open Source Community Contributions
- **OpenWakeWord** - Provides high-quality wake word detection models
- **ONNX Runtime Web** - Enables efficient machine learning model execution in browsers
- **Silero VAD** - Provides accurate voice activity detection capabilities

### Special Thanks
Thanks to all developers and researchers who have contributed to the development of speech processing, machine learning, and web technologies. Without these open-source projects and detailed technical sharing, this project would not have been possible.

## üìÑ License

This project is licensed under the MIT License. See the LICENSE file for details.

---

**ü§ù Contributions Welcome!** If you have any suggestions or find issues, please feel free to submit an Issue or Pull Request.